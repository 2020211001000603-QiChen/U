# UMCé¡¹ç›®å®Œæ•´è¿è¡Œæµç¨‹è¯¦è§£

## ğŸš€ é¡¹ç›®æ•´ä½“æµç¨‹æ¦‚è§ˆ

UMCé¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤šæ¨¡æ€æ— ç›‘ç£èšç±»ç³»ç»Ÿï¼Œä»æ•°æ®å‡†å¤‡åˆ°æœ€ç»ˆç»“æœè¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦é˜¶æ®µï¼š

```
æ•°æ®å‡†å¤‡ â†’ æ¨¡å‹åˆå§‹åŒ– â†’ é¢„è®­ç»ƒ â†’ ä¸»è®­ç»ƒ â†’ æµ‹è¯•è¯„ä¼° â†’ ç»“æœè¾“å‡º
    â†“         â†“         â†“       â†“       â†“        â†“
  æ•°æ®åŠ è½½   å‚æ•°é…ç½®   å¯¹æ¯”å­¦ä¹   èšç±»è®­ç»ƒ  æ€§èƒ½è¯„ä¼°  æŒ‡æ ‡ä¿å­˜
```

## ğŸ“Š è¯¦ç»†æµç¨‹åˆ†æ

### é˜¶æ®µ1ï¼šæ•°æ®å‡†å¤‡å’Œé¢„å¤„ç† (DataManager)

#### 1.1 æ•°æ®åŠ è½½æµç¨‹
```python
# å…¥å£ï¼šrun.py â†’ DataManager(args)
class DataManager:
    def __init__(self, args):
        # 1. è·å–æ•°æ®é›†é…ç½®
        bm = benchmarks[args.dataset]  # MIntRec, MELD-DA, IEMOCAP-DA
        max_seq_lengths, feat_dims = bm['max_seq_lengths'], bm['feat_dims']
        
        # 2. è®¾ç½®åºåˆ—é•¿åº¦å’Œç‰¹å¾ç»´åº¦
        args.text_seq_len, args.video_seq_len, args.audio_seq_len = max_seq_lengths
        args.text_feat_dim, args.video_feat_dim, args.audio_feat_dim = feat_dims
        
        # 3. åŠ è½½å¤šæ¨¡æ€æ•°æ®
        self.mm_data, self.train_outputs = get_data(args, self.logger)
```

#### 1.2 å¤šæ¨¡æ€æ•°æ®å¤„ç†
```python
def get_data(args, logger):
    # 1. è¯»å–æ ‡ç­¾å’Œç´¢å¼•
    train_data_index, train_label_ids = get_indexes_annotations(args, bm, label_list, 'train.tsv')
    dev_data_index, dev_label_ids = get_indexes_annotations(args, bm, label_list, 'dev.tsv')
    test_data_index, test_label_ids = get_indexes_annotations(args, bm, label_list, 'test.tsv')
    
    # 2. åˆå¹¶è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    train_data_index = train_data_index + dev_data_index
    train_label_ids = train_label_ids + dev_label_ids
    
    # 3. åŠ è½½ä¸‰ç§æ¨¡æ€æ•°æ®
    text_data = get_t_data(args, data_args)      # BERTæ–‡æœ¬ç‰¹å¾
    video_data = get_v_a_data(data_args, video_feats_path, args.video_seq_len)  # Swinè§†é¢‘ç‰¹å¾
    audio_data = get_v_a_data(data_args, audio_feats_path, args.audio_seq_len)   # WavLMéŸ³é¢‘ç‰¹å¾
    
    # 4. åˆ›å»ºå¤šæ¨¡æ€æ•°æ®é›†
    mm_train_data = MMDataset(train_label_ids, text_data['train'], video_data['train'], audio_data['train'])
    mm_test_data = MMDataset(test_label_ids, text_data['test'], video_data['test'], audio_data['test'])
    
    return {'train': mm_train_data, 'test': mm_test_data}, train_outputs
```

#### 1.3 æ•°æ®æ ¼å¼è¯´æ˜
```python
# æ¯ä¸ªæ ·æœ¬çš„æ•°æ®ç»“æ„
sample = {
    'text_feats': torch.tensor([...]),      # BERT tokenized features
    'video_feats': torch.tensor([...]),     # Swin video features  
    'video_lengths': torch.tensor([...]),   # Video sequence lengths
    'audio_feats': torch.tensor([...]),     # WavLM audio features
    'audio_lengths': torch.tensor([...]),   # Audio sequence lengths
    'label_ids': torch.tensor([...])        # Ground truth labels (f0or evaluation)
}
```

### é˜¶æ®µ2ï¼šæ¨¡å‹åˆå§‹åŒ–å’Œé…ç½®

#### 2.1 å‚æ•°ç®¡ç†å™¨åˆå§‹åŒ–
```python
# ParamManagerè´Ÿè´£åŠ è½½å’Œé…ç½®æ‰€æœ‰è¶…å‚æ•°
param = ParamManager(args)
args = param.args

# ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
hyper_parameters = {
    # åŸºç¡€è®­ç»ƒå‚æ•°
    'pretrain_batch_size': 128,
    'train_batch_size': 128,
    'num_pretrain_epochs': 100,
    'num_train_epochs': 100,
    'lr_pre': 2e-5,
    'lr': [3e-4],
    
    # UMCç‰¹å®šå‚æ•°
    'base_dim': 256,           # ç»Ÿä¸€ç‰¹å¾ç»´åº¦
    'nheads': 8,               # æ³¨æ„åŠ›å¤´æ•°
    'encoder_layers_1': 1,     # Transformerç¼–ç å™¨å±‚æ•°
    
    # åˆ›æ–°ç‚¹å¼€å…³
    'enable_video_dual': True,      # ConFEDEè§†é¢‘åŒæŠ•å½±
    'enable_audio_dual': True,      # ConFEDEéŸ³é¢‘åŒæŠ•å½±
    'enable_text_guided_attention': True,  # æ–‡æœ¬å¼•å¯¼æ³¨æ„åŠ›
    'enable_self_attention': True,         # è‡ªæ³¨æ„åŠ›æœºåˆ¶
    'enable_clustering_optimization': True, # èšç±»ä¼˜åŒ–
    
    # æ¸è¿›å¼å­¦ä¹ å‚æ•°
    'delta': [0.02],           # é˜ˆå€¼å¢é•¿æ­¥é•¿
    'thres': [0.05],          # åˆå§‹é˜ˆå€¼
    'max_threshold': 0.5,     # æœ€å¤§é˜ˆå€¼
    'min_threshold': 0.05,    # æœ€å°é˜ˆå€¼
}
```

#### 2.2 æ¨¡å‹æ¶æ„åˆå§‹åŒ–
```python
# ModelManagerè´Ÿè´£åˆ›å»ºUMCæ¨¡å‹
model = ModelManager(args)

# UMCæ¨¡å‹åˆå§‹åŒ–è¿‡ç¨‹ï¼š
class UMC(nn.Module):
    def __init__(self, args):
        # 1. åŸºç¡€ç¼–ç å™¨
        self.text_embedding = BERTEncoder(args)
        
        # 2. ç‰¹å¾æŠ•å½±å±‚
        self.text_layer = nn.Linear(args.text_feat_dim, base_dim)
        self.video_layer = nn.Linear(args.video_feat_dim, base_dim)
        self.audio_layer = nn.Linear(args.audio_feat_dim, base_dim)
        
        # 3. ConFEDEåŒæŠ•å½±æ¨¡å—ï¼ˆå¯é€‰ï¼‰
        if self.enable_video_dual:
            self.video_dual_projector = VideoDualProjector(base_dim, base_dim)
        if self.enable_audio_dual:
            self.audio_dual_projector = AudioDualProjector(base_dim, base_dim)
        
        # 4. æ³¨æ„åŠ›æœºåˆ¶
        self.cross_attn_video_layers = nn.ModuleList([...])
        self.cross_attn_audio_layers = nn.ModuleList([...])
        self.text_guided_video_attn = MultiheadAttention(...)
        self.text_guided_audio_attn = MultiheadAttention(...)
        
        # 5. èšç±»ä¼˜åŒ–æ¨¡å—ï¼ˆå¯é€‰ï¼‰
        if self.use_clustering_projector:
            self.clustering_projector = ClusteringProjector(base_dim, base_dim)
        if self.use_clustering_fusion:
            self.clustering_fusion = ClusteringFusion(base_dim, args.num_labels)
```

### é˜¶æ®µ3ï¼šè®­ç»ƒç®¡ç†å™¨åˆå§‹åŒ–

#### 3.1 UMCManageråˆå§‹åŒ–
```python
# UMCManagerè´Ÿè´£æ•´ä¸ªè®­ç»ƒæµç¨‹æ§åˆ¶
method_manager = method_map[args.method]  # è·å–UMCç®¡ç†å™¨
method = method_manager(args, data, model)

class UMCManager:
    def __init__(self, args, data, model):
        self.args = args
        self.data = data
        self.model = model
        
        # æ¸è¿›å¼å­¦ä¹ ä¼˜åŒ–å™¨
        self.progressive_learning = AdaptiveProgressiveLearning(
            initial_threshold=args.thres,
            max_threshold=args.max_threshold,
            min_threshold=args.min_threshold,
            performance_window=3,
            patience=5
        )
        
        # é¢„è®­ç»ƒç®¡ç†å™¨
        self.pretrain_manager = PretrainUMCManager(args, data, model)
```

#### 3.2 æ¸è¿›å¼å­¦ä¹ ç­–ç•¥
```python
class AdaptiveProgressiveLearning:
    def compute_threshold(self, epoch, total_epochs, current_performance, current_loss):
        """è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼"""
        # 1. è®°å½•å†å²æ€§èƒ½
        self._record_history(epoch, current_performance, current_loss)
        
        # 2. è®¡ç®—åŸºç¡€é˜ˆå€¼ï¼ˆSå‹æ›²çº¿å¢é•¿ï¼‰
        base_threshold = self._compute_base_threshold(epoch, total_epochs)
        
        # 3. æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´
        performance_adjustment = self._compute_performance_adjustment()
        
        # 4. æŸå¤±è‡ªé€‚åº”è°ƒæ•´
        loss_adjustment = self._compute_loss_adjustment()
        
        # 5. ç¨³å®šæ€§è°ƒæ•´
        stability_adjustment = self._compute_stability_adjustment()
        
        # 6. ç»¼åˆè®¡ç®—æœ€ç»ˆé˜ˆå€¼
        adaptive_threshold = (base_threshold + 
                            performance_adjustment + 
                            loss_adjustment + 
                            stability_adjustment)
        
        return np.clip(adaptive_threshold, self.min_threshold, self.max_threshold)
```

### é˜¶æ®µ4ï¼šé¢„è®­ç»ƒé˜¶æ®µ (Pretrain)

#### 4.1 é¢„è®­ç»ƒæµç¨‹
```python
def _train(self, args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    if args.pretrain:
        logger.info('Pretraining begins...')
        self.pretrain_manager._train(args)  # æ‰§è¡Œé¢„è®­ç»ƒ
        logger.info('Pretraining is finished...')
    
    # ä¸»è®­ç»ƒé˜¶æ®µ
    logger.info('Training begins...')
    self._train_main(args)
    logger.info('Training is finished...')
```

#### 4.2 é¢„è®­ç»ƒç›®æ ‡
```python
# é¢„è®­ç»ƒé˜¶æ®µä¸»è¦ç›®æ ‡ï¼š
# 1. å¯¹æ¯”å­¦ä¹ ï¼šå­¦ä¹ å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤º
# 2. ç‰¹å¾å¯¹é½ï¼šå¯¹é½ä¸åŒæ¨¡æ€çš„ç‰¹å¾ç©ºé—´
# 3. åˆå§‹åŒ–ï¼šä¸ºåç»­èšç±»è®­ç»ƒæä¾›è‰¯å¥½çš„åˆå§‹åŒ–

# é¢„è®­ç»ƒæŸå¤±å‡½æ•°
def pretrain_loss(self, features, labels):
    # å¯¹æ¯”å­¦ä¹ æŸå¤±
    contrastive_loss = self.contrastive_loss_fn(features, labels)
    
    # ç‰¹å¾å¯¹é½æŸå¤±
    alignment_loss = self.alignment_loss_fn(features)
    
    total_loss = contrastive_loss + alignment_loss
    return total_loss
```

### é˜¶æ®µ5ï¼šä¸»è®­ç»ƒé˜¶æ®µ (Main Training)

#### 5.1 è®­ç»ƒä¸»å¾ªç¯
```python
def _train_main(self, args):
    """ä¸»è®­ç»ƒå¾ªç¯"""
    for epoch in range(args.num_train_epochs):
        # 1. è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        threshold = self.progressive_learning.compute_threshold(
            epoch, args.num_train_epochs, 
            current_performance, current_loss
        )
        
        # 2. è®­ç»ƒä¸€ä¸ªepoch
        epoch_loss = self._train_epoch(epoch, threshold)
        
        # 3. è¯„ä¼°æ€§èƒ½
        performance = self._evaluate()
        
        # 4. æ—©åœæ£€æŸ¥
        if self.progressive_learning.should_early_stop():
            logger.info(f'Early stopping at epoch {epoch}')
            break
        
        # 5. ä¿å­˜æœ€ä½³æ¨¡å‹
        if performance > self.best_performance:
            self.best_performance = performance
            save_model(self.model, args.model_output_path)
```

#### 5.2 å•epochè®­ç»ƒæµç¨‹
```python
def _train_epoch(self, epoch, threshold):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    self.model.train()
    total_loss = 0.0
    
    for batch_idx, batch in enumerate(self.train_dataloader):
        # 1. æ•°æ®å‡†å¤‡
        text_feats = batch['text_feats'].to(self.device)
        video_feats = batch['video_feats'].to(self.device)
        audio_feats = batch['audio_feats'].to(self.device)
        labels = batch['label_ids'].to(self.device)
        
        # 2. å‰å‘ä¼ æ’­
        features, mlp_output, contrastive_loss, clustering_loss = self.model(
            text_feats, video_feats, audio_feats, 
            mode='train-mm', labels=labels
        )
        
        # 3. æŸå¤±è®¡ç®—
        total_loss = (
            contrastive_loss * self.contrastive_weight +
            clustering_loss * self.clustering_weight
        )
        
        # 4. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        # 5. è®°å½•æŸå¤±
        if batch_idx % 100 == 0:
            logger.info(f'Epoch {epoch}, Batch {batch_idx}, Loss: {total_loss.item():.4f}')
    
    return total_loss.item()
```

#### 5.3 æ¨¡å‹å‰å‘ä¼ æ’­è¯¦è§£
```python
def forward(self, text_feats, video_feats, audio_feats, mode=None, labels=None):
    """UMCæ¨¡å‹å‰å‘ä¼ æ’­"""
    # 1. ç‰¹å¾æå–å’Œå½’ä¸€åŒ–
    text_bert = self.text_embedding(text_feats)  # BERTç¼–ç 
    text_feat = self.text_feat_layer(text_bert)  # æŠ•å½±åˆ°base_dim
    text_feat = self.ln_text(text_feat)
    video_seq = self.ln_video(self.video_layer(video_feats))
    audio_seq = self.ln_audio(self.audio_layer(audio_feats))
    
    # 2. ConFEDEåŒæŠ•å½±å¤„ç†ï¼ˆåˆ›æ–°ç‚¹ä¸€ï¼‰
    if self.enable_video_dual:
        video_seq = self.video_dual_projector(video_seq)
    if self.enable_audio_dual:
        audio_seq = self.audio_dual_projector(audio_seq)
    
    # 3. äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
    text_feat_t = text_feat.permute(1, 0, 2)
    video_seq_t = video_seq.permute(1, 0, 2)
    audio_seq_t = audio_seq.permute(1, 0, 2)
    
    x_video = text_feat_t
    x_audio = text_feat_t
    
    for layer in self.cross_attn_video_layers:
        x_video, _ = layer(x_video, video_seq_t, video_seq_t)
    for layer in self.cross_attn_audio_layers:
        x_audio, _ = layer(x_audio, audio_seq_t, audio_seq_t)
    
    # 4. æ–‡æœ¬å¼•å¯¼æ³¨æ„åŠ›ï¼ˆåˆ›æ–°ç‚¹äºŒï¼‰
    if self.enable_text_guided_attention:
        text_guided_video, _ = self.text_guided_video_attn(text_feat_t, x_video, x_video)
        text_guided_audio, _ = self.text_guided_audio_attn(text_feat_t, x_audio, x_audio)
    else:
        text_guided_video = x_video
        text_guided_audio = x_audio
    
    # 5. è‡ªæ³¨æ„åŠ›å±‚ï¼ˆåˆ›æ–°ç‚¹äºŒï¼‰
    combined_features = torch.cat([text_feat_t, text_guided_video, text_guided_audio], dim=0)
    attended_features = combined_features
    if self.self_attention_layers:
        for self_attn_layer in self.self_attention_layers:
            attended_features, _ = self_attn_layer(
                attended_features, attended_features, attended_features
            )
            attended_features = attended_features + combined_features
    
    # 6. ç‰¹å¾åˆ†ç¦»å’Œå½’ä¸€åŒ–
    seq_len = text_feat_t.shape[0]
    enhanced_text_feat = attended_features[:seq_len]
    enhanced_video_feat = attended_features[seq_len:2*seq_len]
    enhanced_audio_feat = attended_features[2*seq_len:]
    
    enhanced_text_feat = self.post_attention_norm(enhanced_text_feat)
    enhanced_video_feat = self.post_attention_norm(enhanced_video_feat)
    enhanced_audio_feat = self.post_attention_norm(enhanced_audio_feat)
    
    # 7. ç‰¹å¾æ± åŒ–å’Œäº¤äº’
    text_bert_cls = text_bert[:, 0]
    text_bert_proj = self.bert_text_layer(text_bert_cls)
    
    if self.use_attention_pooling:
        text_video_enh_pooled = self.attention_pooling(enhanced_video_feat, text_bert_proj)
        text_audio_enh_pooled = self.attention_pooling(enhanced_audio_feat, text_bert_proj)
    else:
        text_video_enh_pooled = enhanced_video_feat.mean(dim=0)
        text_audio_enh_pooled = enhanced_audio_feat.mean(dim=0)
    
    # 8. ç‰¹å¾äº¤äº’
    interaction_input = torch.stack([text_bert_proj, text_video_enh_pooled, text_audio_enh_pooled], dim=1)
    interaction_input = interaction_input.transpose(0, 1)
    interacted_features, _ = self.feature_interaction(interaction_input, interaction_input, interaction_input)
    interacted_features = interacted_features.transpose(0, 1)
    
    text_bert_proj = interacted_features[:, 0]
    text_video_enh_pooled = interacted_features[:, 1]
    text_audio_enh_pooled = interacted_features[:, 2]
    
    # 9. é—¨æ§èåˆ
    if self.enable_gated_fusion:
        enhanced_text = self.gated_fusion(text_bert_proj, text_video_enh_pooled, text_audio_enh_pooled)
    else:
        enhanced_text = torch.cat([text_bert_proj, text_video_enh_pooled, text_audio_enh_pooled], dim=-1)
        enhanced_text = self.fusion_layer(enhanced_text)
    
    # 10. èšç±»ä¼˜åŒ–è·¯å¾„ï¼ˆåˆ›æ–°ç‚¹ä¸‰ï¼‰
    clustering_features = enhanced_text
    clustering_loss = None
    
    if self.enable_clustering_optimization:
        if self.use_clustering_projector:
            clustering_features, center_features = self.clustering_projector(enhanced_text)
        if self.use_clustering_fusion:
            clustering_features, cluster_weights = self.clustering_fusion(clustering_features)
        
        if labels is not None:
            clustering_loss, compactness_loss, separation_loss = self.clustering_loss_fn(
                clustering_features, labels
            )
    
    # 11. å¯¹æ¯”å­¦ä¹ 
    contrastive_features = self.contrastive_proj(enhanced_text)
    contrastive_loss = None
    if labels is not None:
        contrastive_loss = self.contrastive_loss(contrastive_features, labels)
    
    # 12. è¿”å›ç»“æœ
    if mode == 'train-mm':
        mlp_output = self.shared_embedding_layer(enhanced_text)
        return enhanced_text, mlp_output, contrastive_loss, clustering_loss
    else:
        return enhanced_text
```

### é˜¶æ®µ6ï¼šæµ‹è¯•å’Œè¯„ä¼°

#### 6.1 æµ‹è¯•æµç¨‹
```python
def _test(self, args):
    """æµ‹è¯•é˜¶æ®µ"""
    logger.info('Testing begins...')
    
    # 1. åŠ è½½æœ€ä½³æ¨¡å‹
    restore_model(self.model, args.model_output_path)
    
    # 2. æå–ç‰¹å¾
    features = self._extract_features(args)
    
    # 3. èšç±»
    cluster_labels = self._cluster_features(features)
    
    # 4. è¯„ä¼°
    metrics = self._evaluate_clustering(cluster_labels)
    
    logger.info('Testing is finished...')
    return metrics
```

#### 6.2 ç‰¹å¾æå–
```python
def _extract_features(self, args):
    """æå–æµ‹è¯•é›†ç‰¹å¾"""
    self.model.eval()
    features_list = []
    
    with torch.no_grad():
        for batch in self.test_dataloader:
            text_feats = batch['text_feats'].to(self.device)
            video_feats = batch['video_feats'].to(self.device)
            audio_feats = batch['audio_feats'].to(self.device)
            
            # æå–ç‰¹å¾
            features = self.model(text_feats, video_feats, audio_feats, mode='features')
            features_list.append(features.cpu().numpy())
    
    return np.concatenate(features_list, axis=0)
```

#### 6.3 èšç±»å’Œè¯„ä¼°
```python
def _cluster_features(self, features):
    """å¯¹ç‰¹å¾è¿›è¡Œèšç±»"""
    # ä½¿ç”¨K-meansèšç±»
    kmeans = KMeans(n_clusters=self.args.num_labels, random_state=self.args.seed)
    cluster_labels = kmeans.fit_predict(features)
    return cluster_labels

def _evaluate_clustering(self, cluster_labels):
    """è¯„ä¼°èšç±»ç»“æœ"""
    true_labels = self.test_labels
    
    # è®¡ç®—èšç±»æŒ‡æ ‡
    metrics = clustering_score(true_labels, cluster_labels)
    
    return {
        'NMI': metrics['NMI'],
        'ARI': metrics['ARI'], 
        'ACC': metrics['ACC'],
        'FMI': metrics['FMI']
    }
```

### é˜¶æ®µ7ï¼šæ¶ˆèå®éªŒæµç¨‹

#### 7.1 æ¶ˆèå®éªŒé…ç½®
```python
# åœ¨run.pyä¸­å®šä¹‰æ¶ˆèå®éªŒ
ablation_configs = {
    'baseline_traditional': {
        # ç¦ç”¨æ‰€æœ‰åˆ›æ–°ç‚¹
        'enable_video_dual': False,
        'enable_audio_dual': False,
        'enable_text_guided_attention': False,
        'enable_self_attention': False,
        'enable_clustering_optimization': False,
    },
    'confede_only': {
        # ä»…å¯ç”¨ConFEDEåŒæŠ•å½±
        'enable_video_dual': True,
        'enable_audio_dual': True,
        'enable_text_guided_attention': False,
        'enable_self_attention': False,
        'enable_clustering_optimization': False,
    },
    'full_umc_model': {
        # å¯ç”¨æ‰€æœ‰åˆ›æ–°ç‚¹
        'enable_video_dual': True,
        'enable_audio_dual': True,
        'enable_text_guided_attention': True,
        'enable_self_attention': True,
        'enable_clustering_optimization': True,
    },
    # ... æ›´å¤šæ¶ˆèå®éªŒé…ç½®
}
```

#### 7.2 æ¶ˆèå®éªŒæ‰§è¡Œ
```python
def run_all_ablation_experiments(args):
    """è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ"""
    ablation_experiments = [
        'baseline_traditional', 'confede_only', 'text_guided_only', 
        'gated_fusion_only', 'full_confede', 'full_umc_model'
    ]
    
    results = {}
    
    for experiment_name in ablation_experiments:
        # 1. åˆ›å»ºå®éªŒç‰¹å®šçš„args
        exp_args = copy.deepcopy(args)
        exp_args.ablation_experiment = experiment_name
        
        # 2. åº”ç”¨æ¶ˆèå®éªŒé…ç½®
        apply_ablation_config(exp_args, experiment_name)
        
        # 3. æ‰§è¡Œè®­ç»ƒå’Œæµ‹è¯•
        try:
            param = ParamManager(exp_args)
            data = DataManager(exp_args)
            logger = set_logger(exp_args)
            
            work(exp_args, data, logger)
            results[experiment_name] = 'success'
        except Exception as e:
            results[experiment_name] = f'failed: {str(e)}'
    
    return results
```

### é˜¶æ®µ8ï¼šç»“æœè¾“å‡ºå’Œä¿å­˜

#### 8.1 ç»“æœä¿å­˜
```python
def save_results(args, outputs, debug_args=None):
    """ä¿å­˜å®éªŒç»“æœ"""
    results_file = os.path.join(args.results_path, args.results_file_name)
    
    # å†™å…¥CSVæ–‡ä»¶
    with open(results_file, 'a', newline='') as f:
        writer = csv.writer(f)
        
        # å†™å…¥å®éªŒå‚æ•°
        row = [
            args.dataset,
            args.method,
            args.ablation_experiment if hasattr(args, 'ablation_experiment') else 'baseline',
            args.seed,
            outputs['NMI'],
            outputs['ARI'],
            outputs['ACC'],
            outputs['FMI']
        ]
        writer.writerow(row)
```

#### 8.2 æ—¥å¿—è®°å½•
```python
def set_logger(args):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logger = logging.getLogger(args.logger_name)
    logger.setLevel(logging.DEBUG)
    
    # æ–‡ä»¶æ—¥å¿—
    log_path = os.path.join(args.log_path, args.log_id + '.log')
    fh = logging.FileHandler(log_path)
    fh_formatter = logging.Formatter('%(asctime)s - %(message)s')
    fh.setFormatter(fh_formatter)
    logger.addHandler(fh)
    
    # æ§åˆ¶å°æ—¥å¿—
    ch = logging.StreamHandler()
    ch_formatter = logging.Formatter('%(message)s')
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    
    return logger
```

## ğŸ”„ å®Œæ•´è¿è¡Œæµç¨‹æ€»ç»“

### å‘½ä»¤è¡Œæ‰§è¡Œæµç¨‹
```bash
# 1. åŸºç¡€å®éªŒ
python run.py --dataset MIntRec --train --save_results

# 2. æ¶ˆèå®éªŒ
python run.py --dataset MIntRec --ablation_experiment baseline_traditional --train --save_results

# 3. è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ
python run.py --dataset MIntRec --run_all_ablation --save_results

# 4. ä½¿ç”¨è„šæœ¬è¿è¡Œ
sh examples/run_umc.sh
```

### ç¨‹åºæ‰§è¡Œæµç¨‹
```
1. å‚æ•°è§£æ (parse_arguments)
   â†“
2. æ¶ˆèå®éªŒé…ç½® (apply_ablation_config)
   â†“
3. å‚æ•°ç®¡ç†å™¨åˆå§‹åŒ– (ParamManager)
   â†“
4. æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ– (DataManager)
   â†“
5. æ—¥å¿—è®¾ç½® (set_logger)
   â†“
6. æ¨¡å‹åˆå§‹åŒ– (ModelManager)
   â†“
7. è®­ç»ƒç®¡ç†å™¨åˆå§‹åŒ– (UMCManager)
   â†“
8. é¢„è®­ç»ƒé˜¶æ®µ (PretrainUMCManager._train)
   â†“
9. ä¸»è®­ç»ƒé˜¶æ®µ (UMCManager._train_main)
   â†“
10. æµ‹è¯•é˜¶æ®µ (UMCManager._test)
    â†“
11. ç»“æœä¿å­˜ (save_results)
```

### æ•°æ®æµå‘
```
åŸå§‹æ•°æ® (TSVæ–‡ä»¶)
    â†“
æ•°æ®åŠ è½½ (DataManager)
    â†“
å¤šæ¨¡æ€æ•°æ®é›† (MMDataset)
    â†“
æ•°æ®åŠ è½½å™¨ (DataLoader)
    â†“
æ‰¹æ¬¡æ•°æ® (Batch)
    â†“
æ¨¡å‹å‰å‘ä¼ æ’­ (UMC.forward)
    â†“
ç‰¹å¾è¡¨ç¤º (Features)
    â†“
èšç±»ç»“æœ (Cluster Labels)
    â†“
è¯„ä¼°æŒ‡æ ‡ (Metrics)
    â†“
ç»“æœä¿å­˜ (CSVæ–‡ä»¶)
```

### å…³é”®è¾“å‡ºæ–‡ä»¶
```
logs/                    # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ umc_MIntRec_0_2024-01-01-12-00-00.log

models/                  # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ umc_MIntRec_0.pkl

outputs/                 # è¾“å‡ºç»“æœ
â”œâ”€â”€ results.csv          # èšç±»æŒ‡æ ‡ç»“æœ
â”œâ”€â”€ features.npy         # æå–çš„ç‰¹å¾
â””â”€â”€ cluster_labels.npy   # èšç±»æ ‡ç­¾
```

è¿™ä¸ªå®Œæ•´çš„æµç¨‹å±•ç¤ºäº†UMCé¡¹ç›®ä»æ•°æ®å‡†å¤‡åˆ°æœ€ç»ˆç»“æœè¾“å‡ºçš„å…¨è¿‡ç¨‹ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„èŒè´£å’Œè¾“å‡ºï¼Œå½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„å¤šæ¨¡æ€æ— ç›‘ç£èšç±»ç³»ç»Ÿã€‚
