# UMCé¡¹ç›®æ•´ä½“æ•°æ®æµåˆ†æ

## ğŸ“Š é¡¹ç›®æ¦‚è¿°

UMC (Unified Multimodal Clustering) æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ— ç›‘ç£èšç±»ç³»ç»Ÿï¼Œä¸»è¦å¤„ç†æ–‡æœ¬ã€è§†é¢‘å’ŒéŸ³é¢‘ä¸‰ç§æ¨¡æ€çš„æ•°æ®ï¼Œé€šè¿‡åˆ›æ–°çš„èåˆæœºåˆ¶å’Œæ¸è¿›å¼å­¦ä¹ ç­–ç•¥å®ç°é«˜è´¨é‡çš„èšç±»æ•ˆæœã€‚

## ğŸ”„ æ•´ä½“æ•°æ®æµæ¶æ„

```
æ•°æ®è¾“å…¥å±‚ â†’ ç‰¹å¾æå–å±‚ â†’ å¤šæ¨¡æ€èåˆå±‚ â†’ èšç±»å­¦ä¹ å±‚ â†’ è¾“å‡ºç»“æœå±‚
    â†“           â†“           â†“           â†“           â†“
  åŸå§‹æ•°æ®    ç¼–ç ç‰¹å¾     èåˆç‰¹å¾     èšç±»æ ‡ç­¾     è¯„ä¼°æŒ‡æ ‡
```

## ğŸ“‹ è¯¦ç»†æ•°æ®æµåˆ†æ

### 1. æ•°æ®è¾“å…¥å±‚ (Data Input Layer)

#### 1.1 æ•°æ®æº
- **æ–‡æœ¬æ•°æ®**: `train.tsv`, `dev.tsv`, `test.tsv`
  - æ ¼å¼: BERT tokenization
  - ç»´åº¦: `[batch_size, 3, seq_len]` (input_ids, attention_mask, token_type_ids)
  
- **è§†é¢‘æ•°æ®**: `swin_feats.pkl`
  - é¢„æå–çš„Swin Transformerç‰¹å¾
  - ç»´åº¦: `[batch_size, seq_len, 256]`
  
- **éŸ³é¢‘æ•°æ®**: `wavlm_feats.pkl`
  - é¢„æå–çš„WavLMç‰¹å¾
  - ç»´åº¦: `[batch_size, seq_len, 768]`

#### 1.2 æ•°æ®åŠ è½½æµç¨‹
```python
# DataManageråˆå§‹åŒ–
class DataManager:
    def __init__(self, args):
        # 1. è·å–æ•°æ®é›†é…ç½®
        bm = benchmarks[args.dataset]  # MIntRec, MELD-DA, IEMOCAP-DA
        max_seq_lengths, feat_dims = bm['max_seq_lengths'], bm['feat_dims']
        
        # 2. è®¾ç½®åºåˆ—é•¿åº¦å’Œç‰¹å¾ç»´åº¦
        args.text_seq_len, args.video_seq_len, args.audio_seq_len = max_seq_lengths
        args.text_feat_dim, args.video_feat_dim, args.audio_feat_dim = feat_dims
        
        # 3. åŠ è½½å¤šæ¨¡æ€æ•°æ®
        self.mm_data, self.train_outputs = get_data(args, self.logger)
```

### 2. ç‰¹å¾æå–å±‚ (Feature Extraction Layer)

#### 2.1 æ–‡æœ¬ç‰¹å¾æå–
```python
# BERTç¼–ç æµç¨‹
text_feats [B, 3, L] â†’ BERTEncoder â†’ text_bert [B, L, 768] â†’ LinearæŠ•å½± â†’ text_feat [B, L, 256]
```

#### 2.2 è§†é¢‘ç‰¹å¾æå–
```python
# Swinç‰¹å¾å¤„ç†
video_feats [B, L, 256] â†’ Transformerç¼–ç å™¨ â†’ video_seq [B, L, 256]
```

#### 2.3 éŸ³é¢‘ç‰¹å¾æå–
```python
# WavLMç‰¹å¾å¤„ç†
audio_feats [B, L, 768] â†’ Transformerç¼–ç å™¨ + LinearæŠ•å½± â†’ audio_seq [B, L, 256]
```

### 3. å¤šæ¨¡æ€èåˆå±‚ (Multimodal Fusion Layer)

#### 3.1 ConFEDEåŒæŠ•å½±æœºåˆ¶ (åˆ›æ–°ç‚¹ä¸€)
```python
# è§†é¢‘åŒæŠ•å½±
class VideoDualProjector:
    def forward(self, x):
        simi_feat = self.simi_proj(x)      # ä¸»è¦ä¿¡æ¯ (äººç‰©ã€åŠ¨ä½œã€è¡¨æƒ…)
        dissimi_feat = self.dissimi_proj(x) # ç¯å¢ƒä¿¡æ¯ (èƒŒæ™¯ã€åœºæ™¯ã€ç¯å¢ƒ)
        dual_feat = torch.cat([simi_feat, dissimi_feat], dim=-1)
        enhanced_feat = self.fusion(dual_feat)
        return enhanced_feat + x  # æ®‹å·®è¿æ¥

# éŸ³é¢‘åŒæŠ•å½±
class AudioDualProjector:
    def forward(self, x):
        simi_feat = self.simi_proj(x)      # ä¸»è¦ä¿¡æ¯ (è¯­éŸ³å†…å®¹ã€è¯­è°ƒã€æƒ…æ„Ÿ)
        dissimi_feat = self.dissimi_proj(x) # ç¯å¢ƒä¿¡æ¯ (èƒŒæ™¯å™ªéŸ³ã€ç¯å¢ƒéŸ³ã€éŸ³è´¨)
        dual_feat = torch.cat([simi_feat, dissimi_feat], dim=-1)
        enhanced_feat = self.fusion(dual_feat)
        return enhanced_feat + x  # æ®‹å·®è¿æ¥
```

#### 3.2 æ³¨æ„åŠ›æœºåˆ¶
```python
# æ–‡æœ¬å¼•å¯¼äº¤å‰æ³¨æ„åŠ›
text_feat_t (æŸ¥è¯¢) Ã— video_seq_t (é”®å€¼) â†’ x_video
text_feat_t (æŸ¥è¯¢) Ã— audio_seq_t (é”®å€¼) â†’ x_audio

# æ–‡æœ¬å¼•å¯¼æ³¨æ„åŠ› (åˆ›æ–°ç‚¹äºŒ)
text_guided_video = text_guided_video_attn(text_feat_t, x_video, x_video)
text_guided_audio = text_guided_audio_attn(text_feat_t, x_audio, x_audio)
```

#### 3.3 è‡ªæ³¨æ„åŠ›å±‚ (åˆ›æ–°ç‚¹äºŒ)
```python
# ç‰¹å¾æ‹¼æ¥å’Œè‡ªæ³¨æ„åŠ›
combined_features = torch.cat([text_feat_t, text_guided_video, text_guided_audio], dim=0)
attended_features = self_attention_layers(combined_features)
# ç‰¹å¾åˆ†ç¦»
enhanced_text_feat = attended_features[:seq_len]
enhanced_video_feat = attended_features[seq_len:2*seq_len]
enhanced_audio_feat = attended_features[2*seq_len:]
```

#### 3.4 é—¨æ§èåˆæœºåˆ¶
```python
# æƒé‡è®¡ç®—
text_weight = torch.sigmoid(self.text_weight_gate(text_feat))
video_weight = torch.sigmoid(self.video_weight_gate(video_feat))
audio_weight = torch.sigmoid(self.audio_weight_gate(audio_feat))

# åŠ æƒèåˆ
enhanced_text = (text_weight * text_feat + 
                video_weight * video_feat + 
                audio_weight * audio_feat)
```

### 4. èšç±»å­¦ä¹ å±‚ (Clustering Learning Layer)

#### 4.1 é«˜è´¨é‡æ ·æœ¬é€‰æ‹© (ConvexSampler)
```python
class ConvexSampler:
    def sample(self, features, threshold):
        # 1. K-means++åˆå§‹åŒ–èšç±»ä¸­å¿ƒ
        kmeans = KMeans(n_clusters=self.num_clusters, init='k-means++')
        cluster_centers = kmeans.fit(features).cluster_centers_
        
        # 2. è®¡ç®—æ ·æœ¬åˆ°ä¸­å¿ƒçš„è·ç¦»
        distances = self._compute_distances(features, cluster_centers)
        
        # 3. åŠ¨æ€é˜ˆå€¼é€‰æ‹©é«˜è´¨é‡æ ·æœ¬
        select_ids = self._select_high_quality_samples(distances, threshold)
        
        # 4. ç”Ÿæˆä¼ªæ ‡ç­¾
        pseudo_labels = kmeans.predict(features[select_ids])
        
        return select_ids, pseudo_labels
```

#### 4.2 æ¸è¿›å¼å­¦ä¹ ä¼˜åŒ–
```python
class AdaptiveProgressiveLearning:
    def compute_threshold(self, epoch, total_epochs, current_performance, current_loss):
        # 1. åŸºç¡€é˜ˆå€¼ (Så‹æ›²çº¿å¢é•¿)
        base_threshold = self._compute_base_threshold(epoch, total_epochs)
        
        # 2. æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´
        performance_adjustment = self._compute_performance_adjustment()
        
        # 3. æŸå¤±è‡ªé€‚åº”è°ƒæ•´
        loss_adjustment = self._compute_loss_adjustment()
        
        # 4. ç¨³å®šæ€§è°ƒæ•´
        stability_adjustment = self._compute_stability_adjustment()
        
        # 5. ç»¼åˆè®¡ç®—æœ€ç»ˆé˜ˆå€¼
        adaptive_threshold = (base_threshold + 
                            performance_adjustment + 
                            loss_adjustment + 
                            stability_adjustment)
        
        return np.clip(adaptive_threshold, self.min_threshold, self.max_threshold)
```

#### 4.3 æŸå¤±å‡½æ•°ç»„åˆ
```python
# ä¸»è¦æŸå¤±
contrastive_loss = SupConLoss(features, labels)  # ç›‘ç£å¯¹æ¯”å­¦ä¹ 
instance_loss = InstanceLoss(features, labels)   # æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ 

# è¾…åŠ©æŸå¤± (å¯é€‰)
clustering_loss = CompactnessLoss(features, labels) + SeparationLoss(features, labels)

# æ€»æŸå¤±
total_loss = (contrastive_loss * 1.0 + 
             instance_loss * 1.0 + 
             clustering_loss * 0.1)
```

### 5. è®­ç»ƒæµç¨‹ (Training Pipeline)

#### 5.1 é¢„è®­ç»ƒé˜¶æ®µ (å¯é€‰)
```python
def pretrain(self, args):
    # å†»ç»“BERTå‚æ•° (é™¤æœ€å2å±‚)
    freeze_bert_parameters(self.model, args.freeze_bert_parameters)
    
    # å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
    for epoch in range(args.num_pretrain_epochs):
        for batch in self.pretrain_dataloader:
            features = self.model(batch, mode='pretrain-mm')
            loss = self.contrastive_loss_fn(features, batch['labels'])
            loss.backward()
            self.optimizer.step()
```

#### 5.2 ä¸»è®­ç»ƒé˜¶æ®µ
```python
def train_main(self, args):
    for epoch in range(args.num_train_epochs):
        # 1. è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        threshold = self.progressive_learning.compute_threshold(
            epoch, args.num_train_epochs, current_performance, current_loss
        )
        
        # 2. è®­ç»ƒä¸€ä¸ªepoch
        epoch_loss = self._train_epoch(epoch, threshold)
        
        # 3. è¯„ä¼°æ€§èƒ½
        performance = self._evaluate()
        
        # 4. æ—©åœæ£€æŸ¥
        if self.progressive_learning.should_early_stop():
            break
```

#### 5.3 å•epochè®­ç»ƒæµç¨‹
```python
def _train_epoch(self, epoch, threshold):
    for batch_idx, batch in enumerate(self.train_dataloader):
        # 1. å‰å‘ä¼ æ’­
        features, mlp_output, contrastive_loss, clustering_loss = self.model(
            batch['text_feats'], batch['video_feats'], batch['audio_feats'], 
            mode='train-mm', labels=batch['label_ids']
        )
        
        # 2. æŸå¤±è®¡ç®—
        total_loss = (contrastive_loss * self.contrastive_weight +
                     clustering_loss * self.clustering_weight)
        
        # 3. åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
```

### 6. æµ‹è¯•å’Œè¯„ä¼° (Testing & Evaluation)

#### 6.1 ç‰¹å¾æå–
```python
def _extract_features(self, args):
    self.model.eval()
    features_list = []
    
    with torch.no_grad():
        for batch in self.test_dataloader:
            features = self.model(
                batch['text_feats'], batch['video_feats'], batch['audio_feats'], 
                mode='features'
            )
            features_list.append(features.cpu().numpy())
    
    return np.concatenate(features_list, axis=0)
```

#### 6.2 èšç±»å’Œè¯„ä¼°
```python
def _cluster_features(self, features):
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=self.args.num_labels, random_state=self.args.seed)
    cluster_labels = kmeans.fit_predict(features)
    return cluster_labels

def _evaluate_clustering(self, cluster_labels):
    true_labels = self.test_labels
    
    # è®¡ç®—èšç±»æŒ‡æ ‡
    metrics = clustering_score(true_labels, cluster_labels)
    
    return {
        'NMI': metrics['NMI'],    # æ ‡å‡†åŒ–äº’ä¿¡æ¯
        'ARI': metrics['ARI'],    # è°ƒæ•´å…°å¾·æŒ‡æ•°
        'ACC': metrics['ACC'],    # èšç±»å‡†ç¡®ç‡
        'FMI': metrics['FMI']     # Fowlkes-MallowsæŒ‡æ•°
    }
```

## ğŸ”§ å…³é”®æŠ€æœ¯ç‰¹ç‚¹

### 1. åˆ›æ–°ç‚¹æ€»ç»“
- **åˆ›æ–°ç‚¹ä¸€**: ConFEDEåŒæŠ•å½±æœºåˆ¶ï¼Œåˆ†åˆ«æ•è·ä¸»è¦ä¿¡æ¯å’Œç¯å¢ƒä¿¡æ¯
- **åˆ›æ–°ç‚¹äºŒ**: æ–‡æœ¬å¼•å¯¼æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºå¤šæ¨¡æ€äº¤äº’
- **åˆ›æ–°ç‚¹ä¸‰**: èšç±»ä¼˜åŒ–æ¶æ„ï¼ŒåŒ…å«ç´§å¯†åº¦å’Œåˆ†ç¦»åº¦æŸå¤±

### 2. æ•°æ®ç»´åº¦å˜åŒ–
```
è¾“å…¥: æ–‡æœ¬[B,3,L] + è§†é¢‘[B,L,256] + éŸ³é¢‘[B,L,768]
  â†“
ç‰¹å¾æå–: æ–‡æœ¬[B,L,256] + è§†é¢‘[B,L,256] + éŸ³é¢‘[B,L,256]
  â†“
èåˆ: [B,L,256] (ç»Ÿä¸€ç»´åº¦)
  â†“
æ± åŒ–: [B,256] (æœ€ç»ˆç‰¹å¾è¡¨ç¤º)
  â†“
èšç±»: [N] (èšç±»æ ‡ç­¾)
```

### 3. å…³é”®è¶…å‚æ•°
- **å­¦ä¹ ç‡**: 3e-4 (ä¸»è®­ç»ƒ), 2e-5 (é¢„è®­ç»ƒ)
- **æ‰¹æ¬¡å¤§å°**: 128
- **è®­ç»ƒè½®æ•°**: 100
- **æ¸©åº¦å‚æ•°**: 1.4 (ç›‘ç£), 1.0 (æ— ç›‘ç£), 0.07 (å¯¹æ¯”å­¦ä¹ )
- **é˜ˆå€¼èŒƒå›´**: [0.05, 0.5]
- **åŸºç¡€ç»´åº¦**: 256

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. è®­ç»ƒä¼˜åŒ–
- æ¢¯åº¦è£å‰ª: -1.0 (ç¦ç”¨)
- å­¦ä¹ ç‡è°ƒåº¦: çº¿æ€§warmup + ä½™å¼¦é€€ç«
- å‚æ•°å†»ç»“: BERTå‚æ•°å†»ç»“ (é™¤æœ€å2å±‚)
- æ··åˆç²¾åº¦: æ”¯æŒFP16è®­ç»ƒ

### 2. å†…å­˜ä¼˜åŒ–
- ç‰¹å¾ç¼“å­˜: é¢„è®¡ç®—å’Œç¼“å­˜ç‰¹å¾
- åŠ¨æ€å†…å­˜åˆ†é…
- æ‰¹å¤„ç†ä¼˜åŒ–

### 3. è®¡ç®—ä¼˜åŒ–
- å¤šGPUæ”¯æŒ
- æ‰¹å¤„ç†æ¨ç†
- æ¨¡å‹é‡åŒ–æ”¯æŒ

## ğŸ¯ æ¶ˆèå®éªŒæ§åˆ¶

æ¨¡å‹é€šè¿‡å¤šä¸ªå¼€å…³æ§åˆ¶ä¸åŒç»„ä»¶çš„å¯ç”¨/ç¦ç”¨ï¼š
- `enable_video_dual`: æ§åˆ¶è§†é¢‘åŒæŠ•å½±
- `enable_audio_dual`: æ§åˆ¶éŸ³é¢‘åŒæŠ•å½±
- `enable_text_guided_attention`: æ§åˆ¶æ–‡æœ¬å¼•å¯¼æ³¨æ„åŠ›
- `enable_self_attention`: æ§åˆ¶è‡ªæ³¨æ„åŠ›å±‚
- `enable_clustering_optimization`: æ§åˆ¶èšç±»ä¼˜åŒ–
- `use_attention_pooling`: æ§åˆ¶æ³¨æ„åŠ›æ± åŒ–

## ğŸ“Š è¾“å‡ºç»“æœ

### 1. è¯„ä¼°æŒ‡æ ‡
- **NMI**: æ ‡å‡†åŒ–äº’ä¿¡æ¯ [0, 1]
- **ARI**: è°ƒæ•´å…°å¾·æŒ‡æ•° [-1, 1]
- **ACC**: èšç±»å‡†ç¡®ç‡ [0, 1]
- **FMI**: Fowlkes-MallowsæŒ‡æ•° [0, 1]

### 2. è¾“å‡ºæ–‡ä»¶
- `logs/`: è®­ç»ƒæ—¥å¿—
- `models/`: ä¿å­˜çš„æ¨¡å‹
- `outputs/results.csv`: èšç±»æŒ‡æ ‡ç»“æœ
- `outputs/features.npy`: æå–çš„ç‰¹å¾
- `outputs/cluster_labels.npy`: èšç±»æ ‡ç­¾

## ğŸ”„ å®Œæ•´æ‰§è¡Œæµç¨‹

```
1. å‚æ•°è§£æ â†’ 2. æ¶ˆèå®éªŒé…ç½® â†’ 3. å‚æ•°ç®¡ç†å™¨åˆå§‹åŒ– â†’ 4. æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–
    â†“
5. æ—¥å¿—è®¾ç½® â†’ 6. æ¨¡å‹åˆå§‹åŒ– â†’ 7. è®­ç»ƒç®¡ç†å™¨åˆå§‹åŒ– â†’ 8. é¢„è®­ç»ƒé˜¶æ®µ
    â†“
9. ä¸»è®­ç»ƒé˜¶æ®µ â†’ 10. æµ‹è¯•é˜¶æ®µ â†’ 11. ç»“æœä¿å­˜
```

è¿™ä¸ªæ•°æ®æµåˆ†æå±•ç¤ºäº†UMCé¡¹ç›®ä»æ•°æ®è¾“å…¥åˆ°æœ€ç»ˆç»“æœè¾“å‡ºçš„å®Œæ•´æŠ€æœ¯æµç¨‹ï¼ŒåŒ…æ‹¬æ¯ä¸ªé˜¶æ®µçš„å…·ä½“å®ç°ç»†èŠ‚ã€æ•°æ®ç»´åº¦å˜åŒ–ã€å…³é”®ç®—æ³•å’Œä¼˜åŒ–ç­–ç•¥ã€‚æ•´ä¸ªç³»ç»Ÿé€šè¿‡åˆ›æ–°çš„å¤šæ¨¡æ€èåˆæœºåˆ¶å’Œæ¸è¿›å¼å­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†é«˜è´¨é‡çš„æ— ç›‘ç£èšç±»æ•ˆæœã€‚
