# 内存优化解决方案 - "Killed" 错误修复

## 问题诊断

"Killed" 错误通常是由于**内存不足（OOM）**导致的。从您的输出可以看到：
- ✅ 预训练阶段正常完成（100个epoch）
- ❌ 主训练阶段开始时被系统杀死

## 已实施的修复

### 1. 减少 num_workers（已添加）

在脚本中添加了 `--num_workers 2`，从默认的 16 减少到 2，这将：
- 减少约 **87.5%** 的数据加载进程内存占用
- 每个 worker 进程会缓存数据，16个worker 会占用大量内存

## 进一步优化建议

### 方案1：减少批次大小（推荐）

如果仍然内存不足，需要修改配置文件或通过命令行参数减少批次大小：

#### 选项A：修改配置文件（推荐）

创建或修改 `configs/umc_MELD-DA.py`，减少批次大小：

```python
# 在 configs/umc_MELD-DA.py 中修改
'pretrain_batch_size': 32,      # 从128减少到32
'train_batch_size': 32,         # 从128减少到32
'eval_batch_size': 16,          # 从128减少到16
'test_batch_size': 16,          # 从128减少到16
```

#### 选项B：创建内存优化配置

参考 `configs/umc_MIntRec_memory_optimized.py`，创建 `configs/umc_MELD-DA_memory_optimized.py`：

```python
# 关键优化参数
'pretrain_batch_size': 32,
'train_batch_size': 32,
'eval_batch_size': 16,
'test_batch_size': 16,
'base_dim': 128,                # 从256减少到128
'nheads': 4,                    # 从8减少到4
'num_workers': 2,
```

然后使用：
```bash
--config_file_name umc_MELD-DA_memory_optimized
```

### 方案2：一次只运行一个实验

修改脚本，避免同时运行多个实验：

```bash
#!/usr/bin/bash

# 一次只运行一个seed，避免内存竞争
seed=0  # 先运行seed 0

echo "运行种子: $seed"

python run.py \
    --dataset MELD-DA \
    --data_path 'Datasets' \
    --train \
    --save_model \
    --save_results \
    --seed $seed \
    --gpu_id '0' \
    --num_workers 2 \
    --config_file_name umc_MELD-DA
```

运行完一个seed后，再运行下一个。

### 方案3：监控内存使用

在训练前和训练中监控内存：

```bash
# 查看系统内存
free -h

# 监控内存使用（实时）
watch -n 1 free -h

# 查看GPU内存
nvidia-smi -l 1
```

### 方案4：清理内存

在运行前清理内存：

```bash
# 清理系统缓存
sudo sync && sudo sysctl -w vm.drop_caches=3

# 检查是否有其他进程占用内存
ps aux --sort=-%mem | head -10
```

## 内存使用估算

### 当前配置（批次大小128）

```
模型参数: ~100MB
批次数据: 128 × (768+1024+768) × 70 × 4字节 ≈ 9GB
注意力计算: O(128² × 256 × 8) ≈ 2GB
梯度缓存: ~200MB
中间计算: ~2GB
数据加载进程(16个): ~2GB
总内存需求: ~15GB
```

### 优化后（批次大小32, num_workers=2）

```
模型参数: ~100MB
批次数据: 32 × (768+1024+768) × 70 × 4字节 ≈ 2.3GB
注意力计算: O(32² × 256 × 8) ≈ 0.5GB
梯度缓存: ~200MB
中间计算: ~1GB
数据加载进程(2个): ~0.5GB
总内存需求: ~4.5GB
```

**内存减少：约70%** ✅

## 快速修复步骤

### 步骤1：立即修复（已添加num_workers）

脚本已经添加了 `--num_workers 2`，先尝试运行看看是否解决问题。

### 步骤2：如果仍然被杀死

修改配置文件，减少批次大小：

1. 打开 `configs/umc_MELD-DA.py`
2. 找到以下行并修改：

```python
# 修改前
'pretrain_batch_size': 128,
'train_batch_size': 128,
'eval_batch_size': 128,
'test_batch_size': 128,

# 修改后
'pretrain_batch_size': 32,
'train_batch_size': 32,
'eval_batch_size': 16,
'test_batch_size': 16,
```

### 步骤3：如果还是不够

进一步减少：

```python
'pretrain_batch_size': 16,
'train_batch_size': 16,
'eval_batch_size': 8,
'test_batch_size': 8,
```

或者减少模型复杂度：

```python
'base_dim': 128,  # 从256减少
'nheads': 4,      # 从8减少
```

## 验证修复

运行前检查：

```bash
# 1. 检查可用内存
free -h

# 2. 运行单个实验测试
python run.py \
    --dataset MELD-DA \
    --train \
    --save_model \
    --seed 0 \
    --gpu_id '0' \
    --num_workers 2 \
    --config_file_name umc_MELD-DA

# 3. 如果成功，再运行完整脚本
./examples/run_umc.sh
```

## 性能影响

### 批次大小减少的影响

- **内存使用**：减少 75%（128→32）
- **训练速度**：可能略微降低（小批次需要更多迭代）
- **模型性能**：基本不变（通过梯度累积可以保持等效批次大小）

### 建议

如果内存允许，可以：
1. 保持小批次大小（32）
2. 如果代码支持，可以增加 `gradient_accumulation_steps` 来保持等效批次大小

## 总结

✅ **已修复**：添加了 `--num_workers 2`

🔧 **如果需要进一步优化**：
1. 减少批次大小到 32
2. 一次只运行一个 seed
3. 监控内存使用情况

如果问题仍然存在，请告诉我，我会帮您创建内存优化版本的配置文件。

