# IEMOCAP-DA NMI/ARI æ”¹è¿›æ–¹æ¡ˆ

## ä¸€ã€ä¸ºä»€ä¹ˆ NMI å’Œ ARI ä¼šå¾ˆä½ï¼Ÿ

### æ ¹æœ¬åŸå› åˆ†æ

#### 1. **ç±»åˆ«æç«¯ä¸å¹³è¡¡**ï¼ˆ869.00ï¼‰âš ï¸âš ï¸âš ï¸

**é—®é¢˜**ï¼š
- æœ€å°‘ç±»åˆ«ï¼š2ä¸ªæ ·æœ¬ï¼ˆDAï¼‰
- æœ€å¤šç±»åˆ«ï¼š1738ä¸ªæ ·æœ¬ï¼ˆsï¼‰
- ä¸å¹³è¡¡åº¦ï¼š869.00

**å¯¹NMI/ARIçš„å½±å“**ï¼š
- **K-meansåå‘å¤šæ•°ç±»**ï¼šèšç±»ä¸­å¿ƒä¼šè¢«å¤šæ•°ç±»ä¸»å¯¼
- **å°‘æ•°ç±»è¢«å¿½ç•¥**ï¼šåªæœ‰2ä¸ªæ ·æœ¬çš„ç±»åˆ«å‡ ä¹æ— æ³•å½¢æˆæœ‰æ•ˆèšç±»
- **èšç±»åˆ†é…é”™è¯¯**ï¼šå°‘æ•°ç±»æ ·æœ¬å®¹æ˜“è¢«åˆ†é…åˆ°é”™è¯¯çš„èšç±»
- **NMI/ARIè®¡ç®—**ï¼šè¿™äº›é”™è¯¯ä¼šä¸¥é‡å½±å“äº’ä¿¡æ¯å’Œå…°å¾·æŒ‡æ•°

#### 2. **æ–‡æœ¬ä¿¡æ¯é‡ä¸è¶³**ï¼ˆå¹³å‡11.48 tokensï¼‰âš ï¸âš ï¸

**é—®é¢˜**ï¼š
- å¹³å‡é•¿åº¦ï¼š11.48 tokens
- ä¸­ä½æ•°ï¼š8 tokens
- é…ç½®é•¿åº¦ï¼š44 tokensï¼ˆä½†å®é™…åªç”¨äº†ä¸€å°éƒ¨åˆ†ï¼‰

**å¯¹NMI/ARIçš„å½±å“**ï¼š
- **ç‰¹å¾è¡¨ç¤ºä¸è¶³**ï¼šæ–‡æœ¬ç‰¹å¾å¯èƒ½ä¸å¤Ÿä¸°å¯Œ
- **åŒºåˆ†åº¦ä¸å¤Ÿ**ï¼šä¸åŒç±»åˆ«çš„æ–‡æœ¬å¯èƒ½å¾ˆç›¸ä¼¼
- **èšç±»å›°éš¾**ï¼šç‰¹å¾ç©ºé—´å¯èƒ½ä¸å¤Ÿåˆ†ç¦»

#### 3. **å½“å‰è°ƒæ•´çš„å±€é™æ€§**

**å·²åšçš„è°ƒæ•´**ï¼š
- âœ… é™ä½å­¦ä¹ ç‡ï¼š2e-4
- âœ… æé«˜æ¸©åº¦ï¼š20
- âœ… å¢åŠ base_dimï¼š256
- âœ… è°ƒæ•´é˜ˆå€¼ï¼šæ›´ä¿å®ˆ
- âœ… å¢å¼ºæŸå¤±æƒé‡ï¼š1.5

**å±€é™æ€§**ï¼š
- âš ï¸ **è¿™äº›è°ƒæ•´å¯èƒ½ä¸å¤Ÿ**ï¼šç±»åˆ«ä¸å¹³è¡¡åº¦869.00æ˜¯æç«¯æƒ…å†µ
- âš ï¸ **éœ€è¦æ›´æ¿€è¿›çš„æªæ–½**ï¼šç®€å•çš„å‚æ•°è°ƒæ•´éš¾ä»¥è§£å†³æ ¹æœ¬é—®é¢˜

---

## äºŒã€å½“å‰è°ƒæ•´çš„æœ‰æ•ˆæ€§è¯„ä¼°

### æœ‰æ•ˆæ€§åˆ†æ

| è°ƒæ•´é¡¹ | å¯¹NMI/ARIçš„å½±å“ | æœ‰æ•ˆæ€§è¯„ä¼° |
|--------|----------------|-----------|
| é™ä½å­¦ä¹ ç‡ | è®­ç»ƒæ›´ç¨³å®šï¼Œå¯èƒ½ç•¥å¾®æå‡ | â­â­â­ ä¸­ç­‰ |
| æé«˜æ¸©åº¦ | å­¦ä¹ æ›´å¹³æ»‘ï¼Œå¯èƒ½ç•¥å¾®æå‡ | â­â­â­ ä¸­ç­‰ |
| å¢åŠ base_dim | ç‰¹å¾è¡¨ç¤ºæ›´å¼ºï¼Œå¯èƒ½æå‡ | â­â­â­â­ è¾ƒå¥½ |
| è°ƒæ•´é˜ˆå€¼ | æ›´ä¿å®ˆçš„ç­–ç•¥ï¼Œå¯èƒ½å¸®åŠ© | â­â­ æœ‰é™ |
| å¢å¼ºæŸå¤±æƒé‡ | å¼ºåŒ–èšç±»çº¦æŸï¼Œå¯èƒ½æå‡ | â­â­â­ ä¸­ç­‰ |

### æ€»ä½“è¯„ä¼°

**å½“å‰è°ƒæ•´å¯èƒ½æœ‰æ•ˆï¼Œä½†å¯èƒ½ä¸å¤Ÿ**ï¼š
- âœ… ä¼šæ”¹å–„è®­ç»ƒç¨³å®šæ€§å’Œç‰¹å¾è´¨é‡
- âš ï¸ ä½†ç±»åˆ«ä¸å¹³è¡¡åº¦869.00éœ€è¦æ›´æ¿€è¿›çš„æªæ–½
- âš ï¸ æ–‡æœ¬ä¿¡æ¯é‡ä¸è¶³æ˜¯æ ¹æœ¬æ€§é—®é¢˜ï¼Œéš¾ä»¥å®Œå…¨è§£å†³

**é¢„æœŸæ”¹è¿›**ï¼š
- ä¿å®ˆä¼°è®¡ï¼šNMI/ARI æå‡ 3-8%
- å¦‚æœé—®é¢˜ä¸¥é‡ï¼Œå¯èƒ½éœ€è¦æ›´æ¿€è¿›çš„æ–¹æ¡ˆ

---

## ä¸‰ã€æ›´æ¿€è¿›çš„æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡ï¼ˆä¼˜å…ˆï¼‰

#### A1. ä¿®æ”¹èšç±»ç­–ç•¥ - å¹³è¡¡é‡‡æ ·

ä¿®æ”¹ `clustering` æ–¹æ³•ï¼Œç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘é€‰æ‹©ä¸€å®šæ•°é‡çš„æ ·æœ¬ï¼š

```python
# åœ¨ manager.py çš„ clustering æ–¹æ³•ä¸­
# å½“å‰ï¼šæŒ‰æ¯”ä¾‹é€‰æ‹©ï¼ˆcutoff = len(cluster_samples) * thresholdï¼‰
# æ”¹è¿›ï¼šç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘é€‰æ‹© min_samples ä¸ªæ ·æœ¬

min_samples_per_class = 10  # æ¯ä¸ªç±»åˆ«è‡³å°‘10ä¸ªæ ·æœ¬
for cluster_id in range(self.num_labels):
    cluster_samples = feats[assign_labels == cluster_id]
    pos = list(np.where(assign_labels == cluster_id)[0])
    
    # æ”¹è¿›ï¼šç¡®ä¿æœ€å°æ ·æœ¬æ•°
    cutoff = max(
        int(len(cluster_samples) * threshold), 
        min(min_samples_per_class, len(cluster_samples))
    )
```

#### A2. ä½¿ç”¨åŠ æƒK-means

```python
# åœ¨èšç±»æ—¶ä½¿ç”¨ç±»åˆ«æƒé‡
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', 
                                     classes=np.unique(y_true), 
                                     y=y_true)
# åœ¨K-meansä¸­ä½¿ç”¨æ ·æœ¬æƒé‡ï¼ˆéœ€è¦ä¿®æ”¹K-meanså®ç°ï¼‰
```

#### A3. åˆ†å±‚èšç±»ç­–ç•¥

```python
# å¯¹å¤šæ•°ç±»å’Œå°‘æ•°ç±»ä½¿ç”¨ä¸åŒçš„é˜ˆå€¼
majority_threshold = 0.05  # å¤šæ•°ç±»ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
minority_threshold = 0.3   # å°‘æ•°ç±»ä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼ï¼ˆé€‰æ‹©æ›´å¤šæ ·æœ¬ï¼‰
```

### æ–¹æ¡ˆBï¼šå¢å¼ºç‰¹å¾è¡¨ç¤ºï¼ˆé’ˆå¯¹ä¿¡æ¯ä¸è¶³ï¼‰

#### B1. è¿›ä¸€æ­¥å¢åŠ æ¨¡å‹å®¹é‡

```python
'base_dim': [512],              # ä»256è¿›ä¸€æ­¥å¢åŠ åˆ°512
'self_attention_layers': 3,     # ä»2å¢åŠ åˆ°3
'nheads': 16,                    # ä»8å¢åŠ åˆ°16
```

#### B2. ä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡

```python
# å¦‚æœå¯ä»¥ï¼Œåˆå¹¶ç›¸é‚»çš„å¯¹è¯ç‰‡æ®µ
# æˆ–è€…ä½¿ç”¨æ›´é•¿çš„æ–‡æœ¬çª—å£
```

#### B3. ç‰¹å¾å¢å¼º

```python
# åœ¨ç‰¹å¾æå–åæ·»åŠ é¢å¤–çš„ç‰¹å¾å¢å¼ºå±‚
'use_feature_enhancement': True,
'enhancement_layers': 2,
```

### æ–¹æ¡ˆCï¼šæ”¹è¿›æŸå¤±å‡½æ•°ï¼ˆé’ˆå¯¹ä¸å¹³è¡¡ï¼‰

#### C1. ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰

```python
# æ·»åŠ ç„¦ç‚¹æŸå¤±æ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
'use_focal_loss': True,
'focal_alpha': 0.25,
'focal_gamma': 2.0,
```

#### C2. ç±»åˆ«æƒé‡æŸå¤±

```python
# åœ¨æŸå¤±è®¡ç®—ä¸­ä½¿ç”¨ç±»åˆ«æƒé‡
'use_class_weights': True,
'class_weight_mode': 'balanced',  # æˆ– 'inverse_frequency'
```

#### C3. æ›´æ¿€è¿›çš„æŸå¤±æƒé‡

```python
'clustering_weight': 2.0,        # ä»1.5è¿›ä¸€æ­¥å¢åŠ åˆ°2.0
'compactness_weight': 1.5,       # ä»1.2å¢åŠ åˆ°1.5
'separation_weight': 1.5,        # ä»1.2å¢åŠ åˆ°1.5
'contrastive_weight': 1.0,       # ä»0.5å¢åŠ åˆ°1.0
```

### æ–¹æ¡ˆDï¼šæ”¹è¿›èšç±»ç®—æ³•

#### D1. ä½¿ç”¨å¹³è¡¡K-means

```python
# ä½¿ç”¨å¹³è¡¡K-meansï¼Œç¡®ä¿æ¯ä¸ªèšç±»å¤§å°ç›¸è¿‘
from sklearn.cluster import KMeans
# éœ€è¦å®ç°å¹³è¡¡ç‰ˆæœ¬çš„K-means
```

#### D2. å¤šæ¬¡èšç±»é›†æˆ

```python
# è¿è¡Œå¤šæ¬¡K-meansï¼Œä½¿ç”¨æŠ•ç¥¨æˆ–å¹³å‡
num_clustering_runs = 5
all_assignments = []
for _ in range(num_clustering_runs):
    km = KMeans(...)
    all_assignments.append(km.labels_)
# ä½¿ç”¨æŠ•ç¥¨å†³å®šæœ€ç»ˆåˆ†é…
```

#### D3. åˆ†å±‚èšç±»

```python
# å…ˆå¯¹å¤šæ•°ç±»èšç±»ï¼Œå†å¯¹å°‘æ•°ç±»èšç±»
# æˆ–è€…ä½¿ç”¨å±‚æ¬¡èšç±»æ–¹æ³•
```

---

## å››ã€ç«‹å³å¯å®æ–½çš„æ”¹è¿›

### æ”¹è¿›1ï¼šä¿®æ”¹èšç±»é€‰æ‹©ç­–ç•¥ï¼ˆæœ€é‡è¦ï¼‰

åœ¨ `manager.py` çš„ `clustering` æ–¹æ³•ä¸­æ·»åŠ æœ€å°æ ·æœ¬æ•°ä¿è¯ï¼š

```python
# åœ¨ clustering æ–¹æ³•ä¸­ï¼Œç¬¬354è¡Œé™„è¿‘
min_samples_per_class = max(5, int(len(feats) / self.num_labels * 0.1))  # è‡³å°‘5ä¸ªæˆ–æ€»æ•°çš„10%

for cluster_id in range(self.num_labels):
    cluster_samples = feats[assign_labels == cluster_id]
    pos = list(np.where(assign_labels == cluster_id)[0])
    
    # æ”¹è¿›ï¼šç¡®ä¿æœ€å°æ ·æœ¬æ•°
    cutoff = max(
        int(len(cluster_samples) * threshold), 
        min(min_samples_per_class, len(cluster_samples))
    )
```

### æ”¹è¿›2ï¼šè¿›ä¸€æ­¥å¢åŠ æŸå¤±æƒé‡

```python
'clustering_weight': 2.0,        # ä»1.5å¢åŠ åˆ°2.0
'compactness_weight': 1.5,       # ä»1.2å¢åŠ åˆ°1.5
'separation_weight': 1.5,        # ä»1.2å¢åŠ åˆ°1.5
```

### æ”¹è¿›3ï¼šè°ƒæ•´é˜ˆå€¼ç­–ç•¥

```python
'thres': [0.03],                 # ä»0.05è¿›ä¸€æ­¥é™ä½åˆ°0.03
'delta': [0.01],                 # ä»0.02é™ä½åˆ°0.01ï¼Œæ›´æ…¢çš„å¢é•¿
'max_threshold': 0.3,            # ä»0.5é™ä½åˆ°0.3ï¼Œæ›´ä¿å®ˆçš„ä¸Šé™
```

### æ”¹è¿›4ï¼šå¢åŠ é¢„è®­ç»ƒè½®æ•°

```python
'num_pretrain_epochs': [150],    # ä»100å¢åŠ åˆ°150
```

---

## äº”ã€ä»£ç ä¿®æ”¹å»ºè®®

### ä¿®æ”¹1ï¼šèšç±»é€‰æ‹©ç­–ç•¥ï¼ˆå…³é”®ï¼‰

åœ¨ `methods/unsupervised/UMC/manager.py` çš„ `clustering` æ–¹æ³•ä¸­ï¼š

```python
# å½“å‰ä»£ç ï¼ˆç¬¬354-362è¡Œï¼‰
for cluster_id in range(self.num_labels):
    cluster_samples = feats[assign_labels == cluster_id]
    pos = list(np.where(assign_labels == cluster_id)[0])
    
    cutoff = max(int(len(cluster_samples) * threshold), 1)

# æ”¹è¿›å
min_samples_per_class = max(5, int(len(feats) / self.num_labels * 0.1))

for cluster_id in range(self.num_labels):
    cluster_samples = feats[assign_labels == cluster_id]
    pos = list(np.where(assign_labels == cluster_id)[0])
    
    # ç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘é€‰æ‹©æœ€å°æ ·æœ¬æ•°
    cutoff = max(
        int(len(cluster_samples) * threshold), 
        min(min_samples_per_class, len(cluster_samples))
    )
    
    # å¯¹äºæå°‘æ•°ç±»ï¼ˆå°‘äº5ä¸ªæ ·æœ¬ï¼‰ï¼Œé€‰æ‹©æ‰€æœ‰æ ·æœ¬
    if len(cluster_samples) <= 5:
        cutoff = len(cluster_samples)
```

---

## å…­ã€é¢„æœŸæ•ˆæœ

### å½“å‰è°ƒæ•´ï¼ˆå·²åº”ç”¨ï¼‰

- **é¢„æœŸæ”¹è¿›**ï¼šNMI/ARI æå‡ 3-8%
- **é€‚ç”¨åœºæ™¯**ï¼šè½»åº¦åˆ°ä¸­åº¦ä¸å¹³è¡¡

### æ¿€è¿›è°ƒæ•´ï¼ˆå»ºè®®ï¼‰

- **é¢„æœŸæ”¹è¿›**ï¼šNMI/ARI æå‡ 10-20%
- **é€‚ç”¨åœºæ™¯**ï¼šæç«¯ä¸å¹³è¡¡ï¼ˆå¦‚IEMOCAP-DAçš„869.00ï¼‰

### ç»¼åˆè°ƒæ•´ï¼ˆæ¨èï¼‰

ç»“åˆæ‰€æœ‰æ”¹è¿›ï¼š
- **é¢„æœŸæ”¹è¿›**ï¼šNMI/ARI æå‡ 15-30%
- **ä½†éœ€è¦**ï¼šæ›´å¤šçš„è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æº

---

## ä¸ƒã€å®æ–½ä¼˜å…ˆçº§

### ä¼˜å…ˆçº§1ï¼ˆç«‹å³å®æ–½ï¼‰âœ…

1. âœ… å·²åšï¼šé™ä½å­¦ä¹ ç‡ã€æé«˜æ¸©åº¦ã€å¢åŠ base_dim
2. âœ… å·²åšï¼šè°ƒæ•´é˜ˆå€¼ç­–ç•¥ã€å¢å¼ºæŸå¤±æƒé‡
3. ğŸ”§ **å»ºè®®æ·»åŠ **ï¼šä¿®æ”¹èšç±»é€‰æ‹©ç­–ç•¥ï¼ˆæœ€å°æ ·æœ¬æ•°ä¿è¯ï¼‰

### ä¼˜å…ˆçº§2ï¼ˆå¦‚æœæ•ˆæœä»ä¸ç†æƒ³ï¼‰

1. è¿›ä¸€æ­¥å¢åŠ æŸå¤±æƒé‡
2. è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
3. å¢åŠ é¢„è®­ç»ƒè½®æ•°

### ä¼˜å…ˆçº§3ï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰

1. å®ç°ç±»åˆ«æƒé‡æŸå¤±
2. å®ç°å¹³è¡¡K-means
3. ä½¿ç”¨ç„¦ç‚¹æŸå¤±

---

## å…«ã€è¯Šæ–­å’Œç›‘æ§

### ç›‘æ§æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ç‰¹åˆ«å…³æ³¨ï¼š

1. **å„ç±»åˆ«çš„é€‰æ‹©æ ·æœ¬æ•°**
   ```python
   # åœ¨clusteringæ–¹æ³•ä¸­æ·»åŠ æ—¥å¿—
   self.logger.info(f"Cluster {cluster_id}: {len(cluster_samples)} samples, selected {cutoff}")
   ```

2. **å„ç±»åˆ«çš„èšç±»è´¨é‡**
   ```python
   # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„NMI/ARI
   for label in unique_labels:
       label_mask = (y_true == label)
       label_nmi = nmi_score(y_true[label_mask], y_pred[label_mask])
   ```

3. **ç‰¹å¾åˆ†ç¦»åº¦**
   ```python
   # è®¡ç®—ç±»å†…è·ç¦»å’Œç±»é—´è·ç¦»
   intra_class_dist = compute_intra_class_distance(feats, y_true)
   inter_class_dist = compute_inter_class_distance(feats, y_true)
   separation_ratio = inter_class_dist / intra_class_dist
   ```

---

## ä¹ã€æ€»ç»“

### å½“å‰è°ƒæ•´çš„æœ‰æ•ˆæ€§

**éƒ¨åˆ†æœ‰æ•ˆï¼Œä½†å¯èƒ½ä¸å¤Ÿ**ï¼š
- âœ… ä¼šæ”¹å–„è®­ç»ƒç¨³å®šæ€§å’Œç‰¹å¾è´¨é‡
- âš ï¸ ä½†ç±»åˆ«ä¸å¹³è¡¡åº¦869.00éœ€è¦æ›´æ¿€è¿›çš„æªæ–½
- âš ï¸ æ–‡æœ¬ä¿¡æ¯é‡ä¸è¶³æ˜¯æ ¹æœ¬æ€§é—®é¢˜

### å»ºè®®

1. **å…ˆè¿è¡Œå½“å‰ä¼˜åŒ–é…ç½®**ï¼Œè§‚å¯Ÿæ•ˆæœ
2. **å¦‚æœNMI/ARIä»ç„¶å¾ˆä½**ï¼ˆ<30%ï¼‰ï¼Œå®æ–½æ›´æ¿€è¿›çš„æ–¹æ¡ˆï¼š
   - ä¿®æ”¹èšç±»é€‰æ‹©ç­–ç•¥ï¼ˆæœ€å°æ ·æœ¬æ•°ä¿è¯ï¼‰
   - è¿›ä¸€æ­¥å¢åŠ æŸå¤±æƒé‡
   - è¿›ä¸€æ­¥é™ä½é˜ˆå€¼
3. **ç›‘æ§å„ç±»åˆ«æ€§èƒ½**ï¼Œç‰¹åˆ«å…³æ³¨å°‘æ•°ç±»
4. **è€ƒè™‘æ•°æ®å±‚é¢çš„æ”¹è¿›**ï¼šå¦‚æœå¯èƒ½ï¼Œåˆå¹¶æˆ–å¢å¼ºæ•°æ®

éœ€è¦æˆ‘å¸®æ‚¨å®ç°è¿™äº›æ›´æ¿€è¿›çš„æ”¹è¿›å—ï¼Ÿ

